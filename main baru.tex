\documentclass[senior]{IPSstyle}


\Year{2020}
\Month{July}
\Author{44181661-9}

\Title{Analysis on the Usage of Topic Model with Background Knowledge inside Discussion Activity in Industrial Engineering Context}

\Advisor{Professor YOSHIE}

\usepackage{amssymb,amsmath}

\usepackage{array}
\usepackage{mathptmx}
\usepackage{helvet}
\usepackage{courier}
\usepackage{type1cm}

\usepackage{makeidx}
\usepackage{graphicx,subfigure}
\usepackage{multicol}
\usepackage{multirow}
\usepackage[bottom]{footmisc}

\usepackage{mathrsfs}
\usepackage{amssymb,amsmath}
\usepackage{amsfonts}
\usepackage{color}
\usepackage{CJKutf8}
\usepackage{smartdiagram}
\usepackage{caption}

\usepackage{listings}
\usepackage{algorithm,algorithmicx,algpseudocode}
\usepackage[toc,page,title,titletoc,header]{appendix}

\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{longtable}

\DeclareMathOperator*{\otherwise}{otherwise}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\Abstract{Consensus building process for enterprise digital transformation is a significant approach on the implementation of Internet of Things (IoT) solutions through product lifecycle management (PLM). When we improve the consensus building process, it is important to find any latent opinions and hidden dialog patterns analyzing discussion activities by stakeholders. Several approaches have been proposed in forms of instructions and frameworks such as causal model of Consensus Building Theory (CBT) and short-term intensive workshop in strategy planning phase of Product Lifecycle Management (PLM) process. This paper will analyze a new approach to improve consensus building process by summarizing discussion activity. The proposed method is done by performing data augmentation and topic modeling with the help of background knowledge on discussion activity held within industrial engineering context. Our method produces a complete summarization of discussion activity that consists of topic distribution and distribution similarity between topics. We also found that the usage of data augmentation and background knowledge will improve topic quality. We validate our findings to a professional consultant and conclude that our approach gives an adequate contribution towards summarizing discussion activity that might improve consensus building process.}

\Keywords{Topic Model, Background Knowledge, Consensus Building, Product Lifecycle Management, Data Augmentation}

\Acknowledgments{Completing master’s degree is a great and valuable experience for me. Studying abroad gave me a lot of interesting knowledge to take advantage of. I would like to thank Prof. Yoshie and Goto-san who have guided me throughout my master’s thesis. I also would like to thank Rococo, ltd. as my main financial support and the organization that enables me to study in Japan. All The knowledge I retrieved is very important for my next journey and my career. 

During my time I spent in Kitakyushu, I got a lot of love and support from people around me. In this part of the Thesis I would like to thank each one of them. Thanks to my parents and family for taking care of me and letting me choose this path of journey. Thanks to the one and only Miranda that keep supporting and encouraging me during my final months in Kitakyushu. Thank you to my fellow school-mates; Hayden, Palm, and many others from Waseda who stay with me through thick and thin. And finally, thanks to the whole family of PPI Kitakyushu (Indonesian student association in Kitakyushu) that always made me feel like home.

\begin{flushright}
Muhammad Luthfi
\end{flushright}}

\begin{document}
\makepreliminarypages
\singlespace
\frontmatter
\tableofcontents
\listoffigures
\listoftables
\mainmatter
\clearemptydoublepage
\setlength{\baselineskip}{23.0pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Chapter 1
\chapter{Introduction} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Chapter 1

%--------------------------------------------------------------------------
\section{Background and Motivation}
%--------------------------------------------------------------------------

A conventional discussion activity happened when a group of people let out their own opinion with appropriate feedbacks from the other. In industries, discussions are being held in various departments to solve specific problems. We can characterize such discussions as a group of people who shares a same interest aimed to build one single consensus.

Some important features of consensus building process are recording, facilitation, and mediation~\cite{susskind}. Recording in this term stands for creating a physical record of what subject being discussed. Recording can be implemented by recording the whole discussion as a video file or even as simple as taking notes on participant’s utterance. Facilitation in a second hand, help participants work together by providing artefact containing the discussion progress which everyone agrees on. Finally, mediation acts to help opposite parties deal with disagreement. In order to perform mediation, one independent person is needed to resolve disputes with his/her objective point of view.

Consensus building plays a significant role in strategy planning phase of Product Lifecyle Management (PLM) process~\cite{stark}. Strategy planning phase is involving people from various levels and departments in an organization [1], thus justifies the needs of consensus building approach. The practice of consensus building often still have frequent problems. During discussion activities, various stakeholders with different personalities and backgrounds might influence conclusion~\cite{he} which will affect tendency and direction of the discussion~\cite{goto1}.

PLM practice also supports manufacturing companies towards digital transformation~\cite{matt}. As one of the implementations of Internet of Things (IoT) solutions, PLM with its holistic paradigm helped companies to change their internal resources e.g. business processes, product data, and people by taking the benefits of its external resource generated by other IoT solutions. In this research, we propose a better consensus building approach for PLM process which will leads to the advancement of digital transformation

%--------------------------------------------------------------------------
\section{Related works}
%--------------------------------------------------------------------------

Researches related to consensus building have been conducted years ago. In general, researches focused on consensus building can be divided into 2 categories based on their focus point; process model and measurement model. All models are designed under the same goal: to improve consensus quality.

Process model focused on a set of rules that participants should follow under specific circumstances. A research proposed a straightforward approach using a fair, open, and freedom-focused process model~\cite{butler}, meaning that all perspective will be considered equal and all participants will have their freedom to disagree. Another research is focused specifically on a subprocess in consensus building like Consensus Building Theory (CBT)~\cite{briggs} which emphasizes the cause of conflict to investigate what specific matter prevent or support consensus building. Other research is focused on a specific implementation of process model~\cite{goto1} e.g. proposing a short term and intensive workshop activity designed for Product Lifecycle Management (PLM) strategy planning phase which involves multi-party stakeholders. The workshop is intended specifically for discussion under digital transformation for smart, connected engineering field.

Meanwhile, measurement model focused more on the criteria to determine consensus presence. Some popular methods are done by using standard deviation of voting results or using Kendall’s coefficient on voting results~\cite{shepherd}. A recent method showed that a digitized approach can be done by tracking every non-verbal aspects of each participant to determine consensus~\cite{katagiri}. Another digitized approach has been done in 2 steps: inviting external facilitator as one independent figure to direct the course of discussion and doing text mining approach on the dialog data taken from the discussion to evaluate consensus~\cite{goto2}.

%--------------------------------------------------------------------------
\section{Research Problem}
%--------------------------------------------------------------------------

Previous researches discussed on how consensus is being built by considering a lot of variables e.g. time consumed, participants contributions, and conflict resolution method. In another hand, more variables should be valued more such as the objectivity of the final consensus and the latent opinion from each participant. A preliminary study regarding this matter has been conducted~\cite{goto2} but it risks of having biased judgment which will damage consensus building process. We proposed a better text mining approach by utilizing topic model algorithms, a set of data as Background Knowledge, and calculating the convergence rate among topic distributions. Data augmentation is also added in the process to increase data quality.

%--------------------------------------------------------------------------
\section{Organization of Thesis}
%--------------------------------------------------------------------------

In this chapter, we discussed about \underline{fundamental knowledge} of consensus building and PLM, we explained \underline{previous works} related to consensus building, and we discussed about the \underline{undiscovered aspects} on how to improve consensus building process as our hypothesis. Chapter~\ref{chapter_2} will explain the \underline{proposed methodology} we used to perform a digitized text mining approach to improve consensus building process. In Chapter~\ref{chapter_3}, we \underline{conduct the experiment} to figure out the best topic model configuration to be used. Chapter~\ref{chapter_4} is the \underline{implementation and validation} of our findings by quantitative evaluation and qualitative review. Finally, in Chapter~\ref{chapter_5}, we will \underline{summarize} what we have achieved so far and provide \underline{future research directions}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Chapter 2
\chapter{Proposed Method} 
\label{chapter_2}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Chapter 2

In this research, we performed a digitized approach of dialog data from PLM-themed discussion activity sessions using data augmentation, topic model with background knowledge, and distribution similarity. First, the data will be prepared by a simple preprocess method and data augmentation. The clean and augmented data will then be experimented by various topic models and hyperparameters, we picked the best configuration and incorporate it into background-knowledge-backed topic model to generate topic distributions. Then, we will calculate the distribution similarity as convergence rate. Finally, we will compare the effectiveness of our approach to previous research and get a professional consultant to analyze the topic distribution results to have an objective review. To summarize, we will take dialog data of discussion session and transform it into topic distributions, similarity value, and most frequent words (if necessary) from each discussion session to be validated by a professional consultant.

%--------------------------------------------------------------------------
\section{Data Augmentation}
%--------------------------------------------------------------------------

We took a real-life dialog data from discussion sessions which ran for 1-2 hour long. Based on the dataset characteristics in Table~\ref{table 1: dataset characteristics}, the dataset we used is very poor. Compared to the common dataset in topic model research with specialization in short text data, our dataset size is 96.55\% lower in terms of number of documents and 85.96\% lower in terms of corpus size. Hence, we are using data augmentation techniques to improve dataset quality. We expand the Easy Data Augmentation~\cite{wei} by adding additional processes: hypernym replacement and hyponym replacement. Hypernym and hyponym of a word is crucial as we thought the topic mixture of a sentence \textit{s} should be the same with another sentence \textit{s’} who has hypernym/hyponym relation with it.

\begin{table}[h]
%\renewcommand{\arraystretch}{1.3}
\caption{Dataset Characteristics}
\label{table 1: dataset characteristics}
\centering
\small \begin{tabular}{|c|c|c|}
\hline
\textbf{Measures Type}&\textbf{PLM Workshop Dataset}&\textbf{Common Dataset \cite{qiang}}\\
\hline
Total Documents&383&11094  \\
\hline
Corpus Size&686&4887 \\
\hline
Average Length&4.83&7.84 \\
\hline
\end{tabular}
\end{table}

The augmentation process will produce additional sentences that is similar sentence for what it refers to. For example, sentence ‘john eat apple’ will produce at least 2 sentences: ‘john eat apple’ (original sentence) and ‘john consume apple’ (augmented sentence by replacement). Hence, If our dataset is having 100 sentences, it will produce at least 200 sentences as a new dataset.

%--------------------------------------------------------------------------
\section{Topic Model with Background Knowledge}
%--------------------------------------------------------------------------

We tried to mine latent opinion of the dataset using topic model with background knowledge. Topic model is an unsupervised learning approach where we could transform documents into document-to-topic distributions and topic-to-word distributions. In topic model point of view, document is a mixture of topic where topic itself is a mixture of word. The most popular method of topic model is Latent Dirichlet Allocation (LDA)~\cite{blei}, in which, current topic model researches mostly use LDA as baseline method. In LDA-based topic model, the learning process consists of generation process and sampling process. In generative process, the initial document-to-topic distributions and topic-to-word distributions are generated using hyperparameter $\alpha$ and $\beta$. Then, in the sampling process, distributions are evaluated by recalculating it using Gibbs Sampling for each word. The graphical notation of LDA topic model is shown in Fig.~\ref{fig_lda}. Meanwhile, the generation algorithm is shown in Algorithm~\ref{alg:lda} where $K$ is number of topics, $D$ is number of documents, $N_d$ is number of words in document $d$, $\phi_k$ is topic-to-word distribution for topic $k$, $\theta_d$ is document-to-topic distribution for document $d$, $z_{id}$ is topic for the $i^{th}$ word in document $d$, and $w_{id}$ is the $i^{th}$ word in document $d$.

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[scale=0.65]{images/lda.png}}
	\caption{LDA Plate Notation}
\label{fig_lda}
\end{figure}

\begin{algorithm}[h]
\caption{Generation Algorithm of LDA}
\label{alg:lda}
\begin{algorithmic}[1]
\For{$k \in \{1,...,K\}$} 
\State Generate $\phi_k$ \texttildelow{} Dir($\beta$) 
\EndFor
\For{$d \in \{1,...,D\}$}
\State Generate $\theta_d$ \texttildelow{} Dir($\alpha$) 
\EndFor
\For{$i,d$ where $d \in \{1,...,D\}$ and $i \in \{1,...,N_d\}$}
\State Generate $z_{id}$ \texttildelow{} Multinomial($\theta_d$)
\State Generate $w_{id}$ \texttildelow{} Multinomial($\phi_{z_{id}}$)
\EndFor
\end{algorithmic}
\end{algorithm}

We mentioned in previous paragraph that our dataset has relatively smaller size compared to common topic model researches. Hence, we assembled various topic models with specialty in short text as suggested by~\cite{qiang}. The whole list of topic models could be seen in Table~\ref{table 2: topic model experiment} that could be categorized into 4 types. The first type is standard, referred to the baseline topic model which is LDA. The second type, one-topic sampling based, will modify the inference process to sample only one topic per document, meaning all words from a single document will only have one topic label. The third type, global word co-occurrence based, will modify document representation into word-network or set of biterms. Self-aggregation based as the last type will merge several documents into one single pseudo-document and then apply standard-type topic model to it.

\begin{table}[h]
%\renewcommand{\arraystretch}{1.3}
\caption{Topic Model Experiment}
\label{table 2: topic model experiment}
\centering
{\begin{tabular}{|c|c|c|}
\hline
\textbf{No.}&\textbf{Topic Model}&\textbf{Type}\\
\hline
1&LDA\cite{blei}&Standard  \\
\hline
2&Dirichlet Multinomial Mixture (DMM)\cite{yin}&\multirow{4}{*}{\shortstack{One-topic \\sampling based}} \\\cline{1-2}
3&Latent-Feature LDA (LFLDA)\cite{nguyen}& \\\cline{1-2}
4&Latent-Feature DMM (LFDMM)\cite{nguyen}& \\\cline{1-2}
5&Generalized Polya Urn DMM (GPU-DMM)\cite{li}& \\\cline{1-2}
6&GPU Poisson-based DMM (GPU-PDMM)\cite{li2}& \\
\hline
7&Biterm Topic Model (BTM)\cite{cheng}&\multirow{2}{*}{\shortstack{Global word \\co-occurence based}} \\\cline{1-2}
8&Word Network Topic Model (WNTM)\cite{zuo}&\\
\hline
9&Self-aggregate Topic Model (SATM)\cite{quan}&\multirow{2}{*}{\shortstack{Self-aggregation \\based}}\\\cline{1-2}
10&Pseudo-based Topic Model (PTM)\cite{zuo2}& \\
\hline
\end{tabular}}
\end{table}

After the experiment is done, we will decide what is the best topic model, hyperparameters, and the number of sentence augmentation processes to use. After that, we will incorporate the result to a new background-knowledge-backed topic model called Source-LDA~\cite{wood} as the most suitable topic model for our case. In Source-LDA, we can use background knowledge data to influence topic labeling thus improving topic
quality in the process.

%--------------------------------------------------------------------------
\subsection{Dirichlet Multinomial Mixture~\cite{yin}}
%--------------------------------------------------------------------------

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[scale=0.65]{images/dmm.png}}
	\caption{DMM Plate Notation}
\label{fig_dmm}
\end{figure}

The main feature of this topic model is by assuming that each document is sampled by only one topic. Thus, assuming all the words from corpus is sampled under a single multinomial distribution ($\theta$). This assumption is proven to be effective in some cases and a lot of other topic model is proposed based on this assumption.

%--------------------------------------------------------------------------
\subsection{Latent-Feature LDA~\cite{nguyen}}
\label{subsec_lflda}
%--------------------------------------------------------------------------

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[scale=1]{images/LFLDA.png}}
	\caption{LFLDA Plate Notation}
\label{fig_lflda}
\end{figure}

This topic model doesn’t explicitly belong to ‘one-topic assumption’ category. However, this topic model is proposed together with ‘Latent-feature DMM’ so we put it under the same category for the sake of simplicity. The main feature of this topic model is by using external word vectors $\omega$ to perform a better inference on topic of each words. The inference is done by sampling a binary indicator variable from Bernoulli distribution ($S$) to pick a new topic from Dirichlet distribution or latent feature vector.

%--------------------------------------------------------------------------
\subsection{Latent-Feature DMM~\cite{nguyen}}
%--------------------------------------------------------------------------

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[scale=1]{images/LFDMM.png}}
	\caption{LFDMM Plate Notation}
\label{fig_lfdmm}
\end{figure}

This topic model is using the same method as ‘Latent-Feature LDA’. However, this is based on ‘one-topic assumption’ so it is only using a single Dirichlet distribution ($\theta$) to
infer topic of each words.

%--------------------------------------------------------------------------
\subsection{Generalized Polya Urn DMM~\cite{li}}
%--------------------------------------------------------------------------

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[scale=1]{images/GPUDMM.png}}
	\caption{GPUDMM Overview}
\label{fig_gpudmm}
\end{figure}

This topic model uses Generalized Polya Urn (GPU) model to improve topic inference. The GPU model itself is described as follow: for every colored ball picked up from a jar, a couple more balls from the same color will be returned. In this case, this topic model will increase the probability of topic $k$ of words that has close meaning with word $w$ using ‘Promotion Matrix’. The rest of the methods of this topic model is the same with DMM.

%--------------------------------------------------------------------------
\subsection{GPU Poisson-based DMM~\cite{li2}}
%--------------------------------------------------------------------------

\begin{figure}[b]
	\centering
	\fbox{\includegraphics[scale=1]{images/GPUPDMM.png}}
	\caption{GPUPDMM Plate Notation}
\label{fig_gpupdmm}
\end{figure}

The main feature of GPU-PDMM is using Poisson distribution to determine the number of topics that can generate a single document $d$ that is limited by specific threshold. The topics of each document is written as $Z_d$ , the mean value of Poisson distribution is $\lambda$, and the $t_d$ is the number of topics that generate document $d$.

%--------------------------------------------------------------------------
\subsection{Biterm Topic Model~\cite{cheng}}
%--------------------------------------------------------------------------

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[scale=0.375]{images/BTM.png}}
	\caption{BTM Plate Notation}
\label{fig_btm}
\end{figure}

BTM deals with word representation in each document. Instead of single word, BTM treats each document as a collection of biterms where each neighboring word are counted as single biterm ($w_1$,$w_2$). Furthermore, topic inference is done for each biterm meaning that each word will be sampled up to 2 times since each word could have been included in 2 biterms.

%--------------------------------------------------------------------------
\subsection{Word Network Topic Model~\cite{zuo}}
%--------------------------------------------------------------------------

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[scale=0.375]{images/WNTM.png}}
	\caption{WNTM Document Representation}
\label{fig_wntm}
\end{figure}

Unlike BTM, WNTM will change the document representation instead of word. A sliding window will be applied to collect the co-occurrence of word $w$ . Then, a word network will be created with words as vertices and edge between vertices is the co-occurrence frequency of 2 words ($w_i$,$w_j$). Pseudo-document $p$ will be created for each vertex taking every adjacent word as the content.

%--------------------------------------------------------------------------
\subsection{Self-aggregate Topic Model~\cite{quan}}
%--------------------------------------------------------------------------

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[scale=0.375]{images/SATM.png}}
	\caption{SATM Plate Notation}
\label{fig_satm}
\end{figure}

SATM assumes that each document $d$ is a short-text snippet sampled from a longer document $D$ . With this assumption, the generation process is slightly modified to form $\varphi$ (probability of words in long document $D$) while also forming $\phi$ and $\theta$ like standard topic model. Moreover, the inference process also altered to estimate the new topic $z$ and new long document $D$ for each word $w$.

%--------------------------------------------------------------------------
\subsection{Pseudo-based Topic Model~\cite{zuo2}}
%--------------------------------------------------------------------------

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[scale=0.375]{images/PTM.png}}
	\caption{PTM Plate Notation}
\label{fig_ptm}
\end{figure}

PTM assumes that each short text is a snippet sampled from one long document $p_l$. Different with SATM, PTM generates $\phi$ as a multinomial distribution for long documents in a single phase using hyperparameter $\lambda$. Thus, reducing the complexity of the overall algorithm. $\phi$ is again re-estimated in each iteration.

%--------------------------------------------------------------------------
\subsection{Source-LDA~\cite{wood}}
\label{subsec_src}
%--------------------------------------------------------------------------

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[scale=0.375]{images/Source-LDA.png}}
	\caption{Source-LDA Plate Notation}
\label{fig_src}
\end{figure}

The main feature of Source-LDA is adding another probability distribution of words for each background knowledge topics. Topic in each word is inferred from both standard topic feature vector $\phi_k$ and background knowledge topic feature vector $\phi_m$.

%--------------------------------------------------------------------------
\section{Distribution Similarity}
%--------------------------------------------------------------------------

In this step, we aimed to picture the topic distribution into a single value that describes the rate of consensus built (convergence rate). In order to do this, we used distribution similarity calculation using Jensen-Shannon Divergence across all distributions~\cite{aslam}. This concludes the final step of our proposed method.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Chapter 3
\chapter{Experiment} 
\label{chapter_3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Chapter 3

In this chapter, we will describe how the experiment process is done. We conducted 4 steps of experiment consists of Preprocessing, Experiment using Standard Topic Model, Experiment using background knowledge Topic Model, and Implementation (will be explained in the next chapter). The steps also shown in Fig.~\ref{fig_flow}. In the first step, we are focusing on how to prepare the data by doing preprocessing and augmentation process. Experiment using topic models focused on testing combinations of augmentation process, type of topic model, and hyperparameters by looking at their topic coherence value. Finally, implementation step focused on presenting our method by topic distributions and convergence value.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.5]{images/flowchart.png}
	\caption{Experiment Flowchart}
\label{fig_flow}
\end{figure}

%--------------------------------------------------------------------------
\section{Preprocessing and Augmentation}
%--------------------------------------------------------------------------

We had an opportunity to utilize dialog data from requirement decisions (discussion session) of 4 Japanese companies. Firstly, data preprocessing and sentence augmentation is done to clean the data. The comparison of dataset characteristics before and after augmentation is shown in Table~\ref{table 3: augmented dataset characteristics}. We managed to expand the dataset up to 1634.46\% from the original size in terms of number of documents, and up to 145.04\% in terms of corpus size.

\begin{table}[b]
%\renewcommand{\arraystretch}{1.3}
\caption{Augmented Dataset Characteristics}
\label{table 3: augmented dataset characteristics}
\centering
{\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Augmentation}&\textbf{Total Documents}&\textbf{Corpus Size}&\textbf{Average Length}\\
\hline
No Augmentation&383&686&4.83\\
\hline
1 Sentence Augmentation&1017&922&5.00\\
\hline
9 Sentence Augmentation&5085&1519&5.10 \\
\hline
12 Sentence Augmentation&6643&1681&5.10 \\
\hline
\end{tabular}}
\end{table}

Furthermore, the dataset property is presented in Table~\ref{table 4: dataset property}. During dialog data collecting process, 2 types of question were asked. Problem-type question was given at the early stage of discussion while solution-type question is asked at the later stage of discussion. Another property is ’response category’ that referred to participant’s own division, and ’organization level’ that referred to participant’s hierarchical level in the company.

\begin{table}[h]
%\renewcommand{\arraystretch}{1.3}
\caption{Dataset Property}
\label{table 4: dataset property}
\centering
{\begin{tabular}{|c|P{10cm}|}
\hline
\textbf{Property Name}&\textbf{Possible value}\\
\hline
Company ID&\{1,2,3,4\}  \\
\hline
Question Type&\{Problem, Solution\} \\
\hline
\multirow{2}{*}{\shortstack{Response Category}}&\{Information Technology, Corporate Management, Business Process, Human Development\}\\
\hline
Organization Level&\{very low, low, medium, high, very high\} \\
\hline
Opinion&\{short sentence consists around 5 words\} \\
\hline
\end{tabular}}
\end{table}

%--------------------------------------------------------------------------
\section{Experiment using Standard Topic Model}
%--------------------------------------------------------------------------
\begin{figure}[h]
	\centering
	\fbox{\includegraphics[scale=0.65]{images/new-topic1.pdf}}
	\caption{Topic coherence value based on topic model and sentence augmentation process performed on Dialog Data}
\label{fig_tme}
\end{figure}

Following data preprocessing step, topic model experiment is conducted on all topic models in Table~\ref{table 2: topic model experiment}. We used topic coherence value to evaluate topic model performance because our dataset is raw and doesn’t have any golden labels~\cite{qiang}. The result of topic model experiment is shown in Fig.~\ref{fig_tme}. The hyperparameter used in this experiment is number of iteration (1000-2000), $\alpha$ value (0.05-0.3), and $\beta$ value (0.005-0.03). The number shown in the figure is the average of topic coherence value of all possible hyperparameters for each sentence augmentation processes. Based on the number of sentence augmentation, 9 augmentation is not producing significant result while 1 augmentation gives the best and most consistent result. Self-Augment Topic Model (SATM) gives a good overall score regardless the number of sentence augmentation process. However, we are going to choose individual score that yields the best topic coherence result. There are 2 highest score: LDA (1.306) and LFLDA (1.304) both with 1 augmentation process, $\alpha$ value of 0.15, $\beta$ value of 0.01, and 2000 iteration. In the next step, we will treat background knowledge data as additional input to LDA and LFLDA and try to score a better topic coherence.

%--------------------------------------------------------------------------
\section{Experiment using Topic Model with Background Knowledge: LDA approach}
\label{sec_elda}
%--------------------------------------------------------------------------

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[scale=0.65]{images/new-topic2.pdf}}
	\caption{Topic coherence value based on background knowledge (BK) augmentation process}
\label{fig_tme2}
\end{figure}

In this step we are going to use Source-LDA as the baseline for background knowledge-backed topic model. Because Source-LDA is based on LDA, we only modified the hyperparameters in this approach. Based on our proposed method in Chapter~\ref{chapter_2}, we are using PLM-themed discussion activity as our dataset. Hence, PLM topics is used as the background knowledge data. We decided to use PTC Value Roadmap\footnote{http://support.ptc.com/WCMS/files/28837/en/J1051} because it contains many PLM Topics with complete definitions for each topic. The background knowledge dataset held a relatively big size consisting of 26 topics, 1068 unique words, and 145.88 average document length.

Fig.~\ref{fig_tme2} shows the topic coherence value relative to the number of sentence augmentation process applied to background knowledge dataset. The topic coherence value is increasing proportionally with the number of sentence augmentation. The usage of sentence augmentation on background knowledge improves topic coherence up to 3.25\%. We picked 12 sentence augmentation process on background knowledge as the best configuration.

The best configuration we found when using Source-LDA approach is explained as follows: $\alpha$ value of 0.15, $\beta$ value of 0.01, 2000 iteration, 1 sentence augmentation on dialog dataset, and 12 sentence augmentation on background knowledge.

%--------------------------------------------------------------------------
\section{Experiment using Topic Model with Background Knowledge: LFLDA approach}
%--------------------------------------------------------------------------

In this approach, we also using Source-LDA as the baseline. We try to assimilate the characteristics of both LFLDA and Source-LDA making it a Background Knowledge-LFLDA (BK-LFLDA). During this experiment, we are using the configuration described in Section~\ref{sec_elda} that is: $\alpha$ value of 0.15, $\beta$ value of 0.01, 2000 iteration, 1 sentence augmentation on dialog dataset, and 12 sentence augmentation on background knowledge.

The main feature of LFLDA, as described in Section~\ref{subsec_lflda}, is by using external word vectors and sampling a binary indicator to determine the source of generated topics. The inference process for each iteration is shown in Algorithm~\ref{alg:lflda}. Meanwhile, the topic  sampling can be seen in Equation~\ref{c_tslflda} and the categorical sampling (CatE) for latent-feature vectors is seen in Equation~\ref{c_cslflda}. In the equation, $\tau_t$ serves as the latent-feature topic vector of topic $t$ that derives from external word vector $\omega$ and re-estimated for each iteration using MAP estimation\footnote{Taken from Mallet toolkit (McCallum, 2002)}, meanwhile $\lambda$ is the probability of source to generate topics for each word.

\begin{algorithm}[h]
\caption{Inference Process of LFLDA}
\label{alg:lflda}
\begin{algorithmic}[1]
\For{$iteration = 1,2,...$} 
\For{$t \in \{1,2,...,T\}$}
\State $\tau_t$ = MAP($\tau_t | Z,S$) 
\EndFor
\EndFor
\For{$d \in \{1,...,D\}$} 
\For{$i \in \{1,...,N_d\}$}
\State Sample $z_{id}$ from P($z_{id}=t$|$Z_{\neg id},S,\tau,\omega$)
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{equation}
P(z_{id} = t | Z_{\neg id},S,\tau,\omega) = (N^{t}_{d_{\neg i}} + K^{t}_{d_{\neg i}} + \alpha)((1-\lambda)\frac{N^{t,w_{d_i}}_{\neg  d_i}+\beta}{N^{t}_{\neg d_i}+V\beta}+\lambda CatE(w_{d_i}|\tau_t\omega^t))
\label{c_tslflda}
\end{equation}

\begin{equation}
CatE(w_{d_i}|\tau_t\omega^t)= \frac{exp(\tau_t \cdot \omega_w)}{\Sigma_{w' \in W} exp(\tau_t \cdot \omega_{w'})}
\label{c_cslflda}
\end{equation}

In Source-LDA, as described in Section~\ref{subsec_src}, the main feature is the usage of $\delta$ that holds the information of background knowledge and how much each words deviates from our corpus. The sampling algorithm of Soure-LDA is shown in Algorithm~\ref{alg:srclda}. Meanwhile, the topic sampling process can be seen in Equation~\ref{c_tssrclda} and the generation of $\delta$ is shown in Equation~\ref{c_dsrclda} where $X_{t,w}$ resembles the frequency of word $w$ appears in topic $t$ of background knowledge and $\lambda$ serves as the probability how much background knowledge will influence topic distributions. Different with LFLDA, $\lambda$ in Source-LDA is not explicitly instantiated rather than derived from $\mu$ and $\sigma$ that acts as the mean value and standard deviation value of $\lambda$.

\begin{algorithm}[h]
\caption{Inference Process of Source-LDA}
\label{alg:srclda}
\begin{algorithmic}[1]
\For{$iteration = 1,2,...$} 
\For{$d \in \{1,...,D\}$} 
\For{$i \in \{1,...,N_d\}$}
\State Sample $z_{id}$ from P($z_{id}=t$|$Z_{\neg id},\delta$)
\EndFor
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{equation}
P(z_{id}=t|Z_{\neg id},\delta)=(N^{t}_{d_{\neg i}} + K^{t}_{d_{\neg i}} + \alpha)(\frac{N^{t,w_{d_i}}_{\neg  d_i}+\delta_{t,w_{d_i}}}{N^{t}_{\neg d_i}+\Sigma_a^V \delta_{a,t}})
\label{c_tssrclda}
\end{equation}

\begin{equation}
\delta_t = (X_{t,1}^\lambda,X_{t,2}^\lambda,...,X_{t,V}^\lambda) 
\label{c_dsrclda}
\end{equation}

In our approach, we tried to combined both main features of LFLDA and Source-LDA by using LFLDA's topic sampling pattern i.e. using word vectors and binary indicator, and using Source-LDA's $\delta$ to take background knowledge into account. Plate notation of BK-LFLDA is shown in Fig.~\ref{fig_bklflda}. In BK-LFLDA, we completely remove the LDA-style in topic inference e.g. $\beta$ value. The inference process is shown in Algorithm~\ref{alg:bklflda} and the topic sampling can be seen in Equation~\ref{c_tsbklflda}. The categorical sampling for latent-feature vectors and the generation of $\delta$ is still using the same method as their predecessors.

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[scale=0.65]{images/BK-LFLDA.png}}
	\caption{BK-LFLDA Plate Notation}
\label{fig_bklflda}
\end{figure}

\begin{algorithm}[h]
\caption{Inference Process of BK-LFLDA}
\label{alg:bklflda}
\begin{algorithmic}[1]
\For{$iteration = 1,2,...$} 
\For{$t \in \{1,2,...,T\}$}
\State $\tau_t$ = MAP($\tau_t | Z,S$) 
\EndFor
\For{$d \in \{1,...,D\}$} 
\For{$i \in \{1,...,N_d\}$}
\State Sample $z_{id}$ from P($z_{id}=t$|$Z_{\neg id},S,\tau,\omega,\delta$)
\EndFor
\EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\begin{equation}
P(z_{id} = t | Z_{\neg id},S,\tau,\omega,\delta) = (N^{t}_{d_{\neg i}} + K^{t}_{d_{\neg i}} + \alpha)((1-\lambda)\frac{N^{t,w_{d_i}}_{\neg  d_i}+\delta_{t,w_{d_i}}}{N^{t}_{\neg d_i}+\Sigma_a^V \delta_{a,t}}+\lambda CatE(w_{d_i}|\tau_t\omega^t))
\label{c_tsbklflda}
\end{equation}


The experiment was done by calculating topic coherence value and modifying $\lambda$ to see whether $\delta$ or word vectors is significantly influence topic quality. Based on the result in Fig.~\ref{fig_tmbklflda}, we conclude that the mixture of both topic sampling using $\delta$  and word vectors gives the best result using $\lambda$=0.4.

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[scale=0.65]{images/new-topic4.pdf}}
	\caption{Topic Coherence Score of BK-LFLDA based on lambda value}
\label{fig_tmbklflda}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Chapter 4
\chapter{Results and Discussion} 
\label{chapter_4}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Chapter 4

This chapter will explain ‘implementation step’ as described in previous paragraph. First, we will quantitatively compare the quality of topic distributions generated using our proposal to previous research~\cite{goto2}. Then, we will ask a professional consultant to do qualitative evaluation towards our result.

\begin{figure}[h]
	\centering
	\fbox{\includegraphics[scale=0.65]{images/new-topic3.pdf}}
	\caption{Topic Coherence value comparison with previous research}
\label{fig_tme3}
\end{figure}

%--------------------------------------------------------------------------
\section{Quantitative Evaluation: Topic Coherence Value}
%--------------------------------------------------------------------------

We compared our findings with previous research~\cite{goto2} as shown in Fig.~\ref{fig_tme3} using topic coherence value. We improved the result by up to 9.04\% by utilizing data augmentation and background knowledge.

\begin{figure}[b]
	\centering
	\fbox{\includegraphics[scale=0.65]{images/new-topic5.pdf}}
	\caption{Similarity Measurement between topic distributions of Source-LDA w/ custom hyperparameter and BK-LFLDA}
\label{fig_tme5}
\end{figure}

In the next section, we will do qualitative evaluation of the generated topic distributions. However, despite of the highest topic coherence score was achieved by BK-LFLDA, we feel that it is better to use Source-LDA with custom hyperparameters as it has been tested in baseline dataset e.g. Wikipedia, Reuters, GoogleNews, etc. Using JS Divergence, we also show how much of difference occurs between topic distributions generated by Source-LDA with custom hyperparameters and topic distributions generated BK-LFLDA in Fig.~\ref{fig_tme5}. The resuls shows that there is not much difference in terms of topics distributions with similarity value above 95\% for each company.

%--------------------------------------------------------------------------
\section{Qualitative Evaluation: Review by Professional Consultant}
%--------------------------------------------------------------------------

We implemented our model to create topic distributions of the dataset. Then, we calculate the convergence rate among topic distributions and do qualitative evaluation together with external facilitator.

During topic modeling process, we replaced background knowledge’s topic name with its topic number to increase readability. The mapping of topic number with its actual name can be seen at Table~\ref{table 5: mapping plm} with a side note that the order of topic number is in alphabetical order and different with what shown in the reference (PTC Value Roadmap). There are 26 PLM Topics in total that serves as best practice for a specific business unit.

%\begin{table}[h]
%\renewcommand{\arraystretch}{1.3}
%\centering
%{
\begin{longtable}{|c|c|}
\caption{Mapping of PLM Topics}\\
\hline
\textbf{Topic No.}&\textbf{PLM Topics}\\
\hline
Topic 0&Business System Support  \\
\hline
Topic 1&Change and Configuration Management \\
\hline
Topic 2&Component and Supplier Management \\
\hline
Topic 3&Concept Development \\
\hline
Topic 4&Design and Manufacturing Outsourcing \\
\hline
Topic 5&Equipment Monitoring and Lifecycle Management \\
\hline
Topic 6&Manufacturing Process Management \\
\hline
Topic 7&Mechanical, Electrical, and Software Development \\
\hline
Topic 8&Performance Analysis and Feedback \\
\hline
Topic 9&Platform Design and Variant Generation \\
\hline
Topic 10&Product Cost Management \\
\hline
Topic 11&Product Support Analysis and Planning \\
\hline
Topic 12&Project Management \\
\hline
Topic 13&Quality and Reliability Management \\
\hline
Topic 14&Regulatory and Materials Compliance \\
\hline
Topic 15&Requirements Definition and Management \\
\hline
Topic 16&Service Diagnostics and Knowledge Management \\
\hline
Topic 17&Service Logistics and Network Management \\
\hline
Topic 18&Service Order Management and Field Service \\
\hline
Topic 19&Service Parts Planning and Pricing \\
\hline
Topic 20&Smart, Connected Product Enablement \\
\hline
Topic 21&System Architecture Design \\
\hline
Topic 22&Technical and Service Parts Information Creation and Delivery\\
\hline
Topic 23&Tooling Design and Manufacture \\
\hline
Topic 24&Verification and Validation \\
\hline
Topic 25&Warranty and Performance-based Contract Management \label{table 5: mapping plm}\\\hline
\end{longtable}
%}
%\end{table}

The average topic distribution of sentences in dialog data from each company are shown in Fig.~\ref{fig_c1}, Fig.~\ref{fig_c2}, Fig.~\ref{fig_c3}, and Fig.~\ref{fig_c4}. The most probable and least probable topic is different for each company. For example, company 1 that runs in business process innovation industry discussed a lot about topic \#12 (Project Management) and very little about topic \#24 (Verification and Validation). Meanwhile, Company 2 from automotive manufacturer industry had a huge interest in topic \#18 (Service Order Management and Field Service) but not in topic \#22 (Technical and Service Parts Information Creation and Delivery). Company 3 from aqua industry has topic \#19 (Service Parts Planning and Pricing) as the most probable topic and topic \#2 (Component and Supplier Management) as the least probable topic. Lastly, company 4 from automotive supplier industry had topic \#22 and topic \#21 (System Architecture Design) as their most probable topics.

Given the topic distributions, we will conduct similarity measurement using JS Divergence to find convergence rate. The interpretation value of JS divergence ranges from 0-1. All value approaching to 0 means that there is no variation between probability distributions, meanwhile value approaching to 1 means that there is high variation between probability distributions. The convergence of each discussion sessions can be seen at Table~\ref{table 6: similarity} along with the top frequent words. The overall convergence rate achieved from each discussion is probably not too good since each one has convergence rate above 0.500. The lowest degree of convergence was achieved by Company 1 with 0.865 while the highest degree of convergence was achieved by Company 3 with 0.672. The top words from each discussion acts as a support to better understand the discussion.

\begin{table}[h]
%\renewcommand{\arraystretch}{1.3}
\caption{Convergence Rate and Top Words}
\label{table 6: similarity}
\centering
{\begin{tabular}{|c|c|c|}
\hline
\textbf{Company ID}&\textbf{Convergence Rate}&\textbf{Top words}\\
\hline
Company 1&0.865&\{Information, Product, Data\}  \\
\hline
Company 2&0.766&\{Production, Work, Product\} \\
\hline
Company 3&0.672&\{Resource, Human, Product, Development\} \\
\hline
Company 4&0.753&\{Information, Data, Sharing\} \\
\hline
\end{tabular}}
\end{table}

Given the results, here is the feedback from professional consultant for each discussion session:

\begin{figure}[h]
    \begin{center}
    \fbox{\includegraphics[width=0.97\linewidth]{images/new-company1.pdf}}
    \end{center}
\vspace{-0.3cm}
    \caption{Average Topic distributions for company 1}
    \label{fig_c1}
    \vspace{-0.3cm}
\end{figure} 

\paragraph{Company 1} Company 1 had a key problem in terms of information exchange between design and manufacturing. I agree that the frequency of Design and Manufacturing topics was high. However, the topic of Project Management was rarely spoken directly by their voices. In addition, the analysis results show that there are few topics on Manufacturing Process Management. Certainly, there were few remarks on Manufacturing Process Management when the workshop was held. However, one of the participants was very concerned about the topic and he is one of the important people in the PLM project, so even if it is a minority opinion, I cannot ignore it as my consultant perspective. By the way, in the analysis results, the words with the highest frequency of occurrence were Information, Product, and Data. These were key words that participants often talked about during the actual workshop. As a consultant, I agree with that.

\begin{figure}[h]
    \begin{center}
    \fbox{\includegraphics[width=0.97\linewidth]{images/new-company2.pdf}}
    \end{center}
\vspace{-0.3cm}
    \caption{Average Topic distributions for company2}
    \label{fig_c2}
    \vspace{-0.3cm}
\end{figure} 

\paragraph{Company 2} The company 2 had three business unit. Thus, the participants had different opinions, as each business unit had a completely different product and each business model was different. When I looked at the results of this analysis, I thought that the reason that the topic of Verification and Validation was high was probably that they had a problem with their product quality. However, although the topic about Field Service has not been talked about in the actual workshop time, the frequency of topic 18 was high in this analysis result. In fact, this company does little field service work, so it is necessary to confirm why such analysis results were performed. In addition, the analysis results indicated that the frequency of Product Cost Management and Project Management topics was low. However, I think the discussions about costs and projects were relatively common during the actual discussions with them. Regarding the word distribution, the analysis result, it showed that the frequency of Production, Work, and Product were high. I agree with this result.

\begin{figure}[h]
    \begin{center}
    \fbox{\includegraphics[width=0.97\linewidth]{images/new-company3.pdf}}
    \end{center}
\vspace{-0.3cm}
    \caption{Average Topic distributions for company 3}
    \label{fig_c3}
    \vspace{-0.3cm}
\end{figure} 

\paragraph{Company 3} The motivation for Company 3 to introduce PLM was to strengthen its field service operations. Looking at the analysis results, it was found that the topics with the highest frequency were field services, such as Warranty management, Performance Based Contract Management, Technical Service Parts Information, and Service Order Management. I agree this result as a professional consultant. However, regarding the monitoring and management of equipment, it was analyzed that the topic frequency was low. This is different from the actual situation, because in the actual workshop, the story of equipment monitoring was relatively well discussed. The frequency of words of Resource, Human, Product, and Development is high. Even during the actual workshop discussion, the shortage of human resources in field service was very problematic. Thus, I agree with the analysis results.

\begin{figure}[h]
    \begin{center}
    \fbox{\includegraphics[width=0.97\linewidth]{images/new-company4.pdf}}
    \end{center}
\vspace{-0.3cm}
    \caption{Average Topic distributions for company 4}
    \label{fig_c4}
    \vspace{-0.3cm}
\end{figure} 

\paragraph{Company 4} Company 4 has been practicing efforts to make its factory a smart factory. As a consultant, what I noticed in their actual workshops was their lack of information sharing between departments and insufficient training of employees. On the other hand, looking at the results of this analysis, we found that the topic \# 22 was Technical and Service Part Information Creation and Delivery. At first, I wasn’t interested in topic \# 22. However, after reviewing the content of discussions with the workshop participants later, there was an opinion that attention was paid to the management of service parts in order to contribute to sustainable sales. It seems that the results of this analysis have taught me a topic that I did not notice at first. Looking at the analysis results of the word distribution, it seems that three words, Information, Data, and Sharing, appear frequently. This was exactly the issue that was being talked about at the workshop. Additionally, the analysis results seem to indicate that there is no relationship between education and system design. Further investigation is needed as I think education topic should be highly related in the workshop.
\bigskip

Based on our analysis and feedback from professional consultant. We feel that our experiment on the usage of topic model with background knowledge in an industrial engineering discussion activity (in this case, PLM-themed) gives an actual contribution towards discussion summarization in which, might improve consensus building process. The important takeaway of this research is that topic modeling with background knowledge will assist professional consultant to understand more towards participant’s latent opinion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Chapter 5
\chapter{Conclusion} 
\label{chapter_5}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Chapter 5

In this paper, we analyzed a new digitized approach to improve consensus building process in discussion activity held within industrial engineering context (PLM-themed). Our proposed method consists of performing data augmentation, implementing topic model with background knowledge, and calculating the distribution similarity. Finally, we validate the result on professional consultant. We received good feedback which validates our purpose of using a new approach to improve consensus building process. We also found that using data augmentation and background knowledge in topic modeling will improve its topic quality. Moreover, BK-LFLDA as our proposed model also yields a better result compared to previous research when using PLM dialog data and PLM background knowledge. 

However, further approach is still necessary based on two perspective: consensus building and topic modeling. From consensus building perspective, we still need to assure the emotional state of discussion participants when dialog data is recorded. Some variables might aspect the quality and consistency of participant’s opinion. Meanwhile from topic modeling perspective, we still need to test BK-LFLDA more to validate that our proposal beats state-of-the-art method. Furthermore, we are planning to expand Source-LDA so it can afford different data representation like BTM and WNTM does.

\bibliographystyle{plain}
\bibliography{references}

\end{document}